# MVP 产品需求文档（无数据库版）

## 一、产品目标
用户上传一段解说/讲话音频，系统自动识别内容、翻译并生成目标语言配音音频，确保与原始时间线基本同步。处理完成后即可播放和下载配音音频与字幕。

---

## 二、系统特点（约束）

- ❌ 无用户注册登录
- ❌ 无数据库存储（不记录历史任务、不保存上传文件）
- ✅ 全部处理过程在用户上传后即时完成
- ✅ 用户在处理页面中即可看到处理结果
- ✅ 任务完成前不可关闭浏览器（或使用前端提示）

---

## 三、核心功能模块（简化版）

### ✅ 上传与参数设置
- 上传音频文件（mp3/wav）
- 选择源语言
- 选择目标语言
- 点击“开始处理”按钮

### ✅ 后台任务流（自动触发）
1. Whisper 识别原始音频（带时间戳字幕）
2. 翻译字幕为目标语言（调用 GPT 或 DeepL）
3. 逐句 TTS 合成目标语言语音
4. 对齐合成音频到原字幕时间（通过拉伸/压缩）
5. 合并音频段并返回最终音频 + 翻译字幕

### ✅ 结果页展示
- 原音频播放器 + 翻译配音播放器
- 中英字幕对照（带时间戳）
- 下载按钮（配音音频 / 字幕文件）

---

## 四、界面结构建议（Next.js 页面）

| 页面路径 | 功能 |
|----------|------|
| `/`      | 首页介绍 + 开始使用按钮 |
| `/upload` | 上传音频、语言选择、提交处理 |
| `/processing` | 显示处理进度条和状态 |
| `/result` | 展示播放器、字幕、下载结果 |

---

## 五、关键 API 流程（前端直传+后台即时处理）

### 📤 上传处理（前端）
- `POST /api/handle-audio`
- 请求体：
  - 音频文件（FormData）
  - 原语言
  - 目标语言

### 🔁 后端处理流程（串行或异步）
1. 调用 Whisper API 获取字幕（含时间戳）
2. 翻译字幕文本（调用 GPT 或 DeepL）
3. 调用 TTS（如 ElevenLabs）生成每句目标语言音频
4. 对齐时长（使用 FFmpeg / librosa）
5. 拼接生成最终音频文件
6. 返回：
   - 翻译字幕（SRT 或 JSON）
   - 合成配音音频（MP3）

### 📥 响应内容
- 返回 Base64 或下载链接（可保留临时文件 30分钟）
- 前端展示音频播放与字幕预览

---

## 六、用户流程图

```plaintext
[上传音频页面]
   ↓
提交（选择语言 + 音频） → 后端处理任务启动
   ↓
[处理中页面]
   ↓
[结果展示页面]
- 播放器
- 字幕中英对照
- 下载按钮

## 七、技术实现建议（无数据库版本）

功能
技术选型
前端
Next.js + Tailwind
上传处理
Next.js API Routes (/api/handle-audio)
Whisper识别
OpenAI Whisper API
翻译
GPT-4 或 DeepL API
TTS 配音
ElevenLabs API
音频合成对齐
ffmpeg 调用 + Python（pydub/librosa）
临时文件存储
内存 / Cloudflare R2 / 本地 TMP
部署推荐
Vercel + Serverless Function（或 AWS Lambda + S3）

## 八、注意事项（MVP）

•	所有处理过程应在 2~5 分钟内完成，控制单个任务时长
•	文件在服务端保留时间不超过 30 分钟（可配置 auto-delete）
•	前端需提示：“请勿关闭窗口，任务处理约需几分钟”